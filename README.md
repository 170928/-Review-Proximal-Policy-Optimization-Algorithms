# -Review-Proximal-Policy-Optimization-Algorithms
[Review]
> John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov  
> OpenAI  
> https://arxiv.org/pdf/1707.06347.pdf  
[Reference for git]  
> https://medium.com/@jonathan_hui/rl-the-math-behind-trpo-ppo-d12f6c745f33

## [Mathematics]
> ì´ repositoryì—ì„œëŠ” í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìˆ˜í•™ì ì¸ í•¨ìˆ˜ë“¤ì— ëŒ€í•´ì„œ ê³µë¶€í•˜ê³  ì ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤.  
> ì¶”í›„ì— GAIL ê³¼ TRPO ì—ì„œë„ ê·¼ê°„ì´ ë˜ëŠ” ìˆ˜ì‹ë“¤ì´ë¯€ë¡œ PPO ë…¼ë¬¸ì˜ ì´í•´ëŠ” ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.  

ê¸°ë³¸ì ìœ¼ë¡œ Reinforcement Learningì€ discounted expected rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.  
ì´ expected discounted rewardë¥¼ Î· (ì—íƒ€) ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46075159-986f9380-c1c4-11e8-8cd5-48616389ce29.png)

### MM Algorithm

PPO ì™€ TRPO ëŠ” Minorize-Maximization (MM) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì‹ì´ êµ¬ì„± ë©ë‹ˆë‹¤.  
ì´ ì•Œê³ ë¦¬ì¦˜ì€ "iterative method"ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•©ë‹ˆë‹¤. 
MM ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ë§¤ iteration ë§ˆë‹¤ ìœ„ ê·¸ë¦¼ì˜ "íŒŒë€ìƒ‰ ì„ "ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” surrogate function M ì„ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.  

ì´ "surrogate function"ì´ ê°–ëŠ” ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
(1) expected discounted reward Î· (ì—íƒ€) ì˜ "lower bound function"  
(2) í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ì •ì±… (policy)ë¥¼ ë”°ë¼ ê·¼ì‚¬í•œ Î· (ì—íƒ€)   
(3) ìµœì í™” (optimize) í•˜ê¸° ì‰¬ìš´ í•¨ìˆ˜  
> optimizeê°€ ì‰¬ìš´ ì´ìœ ëŠ” ì´ surrogate functionì„ quadratic equationìœ¼ë¡œ ê·¼ì‚¬í•  ê²ƒì´ê¸° ë•Œë¬¸ ...  

ë§¤ MM ì•Œê³ ë¦¬ì¦˜ì˜ iteration ë§ˆë‹¤ optimal point M ì„ ì°¾ìŠµë‹ˆë‹¤.  
ê·¸ë¦¬ê³ , point Mì„ "í˜„ì¬ ì‚¬ìš©í•  ì •ì±… (policy)"ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46075518-a5d94d80-c1c5-11e8-86fa-47498fa061f8.png)

ì–»ê²Œ ëœ M ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ lower boundë¥¼ ë‹¤ì‹œ ê³„ì‚° í•˜ë©° ì´ ê³¼ì •ì„ ê³„ì† ë°˜ë³µí•˜ëŠ” ê²ƒì´ MM ì•Œê³ ë¦¬ì¦˜ ì…ë‹ˆë‹¤.  
MM ì•Œê³ ë¦¬ì¦˜ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œì¨ ì •ì±… (policy)ë¥¼ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë˜ì–´ ê°‘ë‹ˆë‹¤.  

### Objective Function

![image](https://user-images.githubusercontent.com/40893452/46075612-fea8e600-c1c5-11e8-9e1c-625051e8234c.png)

ìœ„ì˜ objective functionì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
(1) adavantage function (*expected reward minus baseline to reduction variance*)ì„ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.  
(2) ì—…ë°ì´íŠ¸ í•˜ëŠ” ìƒˆ ì •ì±… (policy)ê°€ í•™ìŠµ ì´ì „ì˜ ì •ì±… (old policy)ë¡œë¶€í„° ë„ˆë¬´ í¬ê²Œ ë³€í™”í•˜ì§€ ì•Šë„ë¡ (not too different) ì œí•œ í•©ë‹ˆë‹¤.  

ìˆ˜ì‹ì—ì„œ ì‚¬ìš©ë˜ëŠ” notationë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ë˜ë©°, ì¼ë°˜ì ì¸ ê°•í™”í•™ìŠµ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ë˜ì–´ ì˜¤ë˜ ê°œë…ì´ ê·¸ëŒ€ë¡œ ì ìš©ë©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46076239-efc33300-c1c7-11e8-9702-24678c598ac0.png)

advantageì˜ ìˆ˜ì‹ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” 2ê°€ì§€ì˜ ë‹¤ë¥¸ ì •ì±… (policy)ë¥¼ ì‚¬ìš©í•´ì„œ í•œìª½ì˜ policyì˜ rewardë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46076881-e0dd8000-c1c9-11e8-8cb8-f073f740ff3f.png)

ìœ„ì˜ ê³¼ì •ì„ í†µí•´ì„œ í˜„ì¬ trajectoriesë¥¼ ë§Œë“œëŠ” phi' ì •ì±…ê³¼ baselineì„ êµ¬ì„±í•˜ëŠ” phi ì •ì±…ê°„ì˜ ê´€ê³„ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ìµœì¢… ê²°ê³¼ì—ì„œ Î·(phi)ë¥¼ ìš°ë³€ìœ¼ë¡œ ë„˜ê²¨ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46076907-f3f05000-c1c9-11e8-8522-3ae2427598e8.png)

expectation (ê¸°ëŒ“ê°’) advantageëŠ” ìš°ë³€ì˜ sigma_s p(s) * sigma_a phi(a|s) * A(s, a) ë¡œ ë³€í™˜ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ì•ì˜ ë‘ sigmaë¡œ ë¬¶ì¸ ë¶€ë¶„ë“¤ì€ í™•ë¥ ì´ë©°, í•´ë‹¹ í™•ë¥ ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ê°’ì„ A(s, a)ë¡œ ë³´ë©´  
í”íˆ ì´í•´í•  ìˆ˜ ìˆëŠ” expectationì— ëŒ€í•œ ìˆ˜ì‹ì´ êµ¬ì„±ë©ë‹ˆë‹¤.  

### Function ğ“›

MM ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” í˜„ì¬ ì •ì±… (current policy)ì—ì„œ Î· (ì—íƒ€) expected discounted rewardë¥¼ ê·¼ì‚¬í•˜ëŠ” ê²ƒìœ¼ë¡œ lower boundë¥¼ ì°¾ê³ ì í•©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46077127-bdff9b80-c1ca-11e8-99c9-b2f149c77160.png)

ê·¸ëŸ¼ function Lì€ function Mì˜ lower bound ì¤‘ ì¼ë¶€ê°€ ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46077159-db346a00-c1ca-11e8-936c-0e5264bf066b.png)


### Surrogate Function


## [Motivation]
 
